# Bounding Box Feature Implementation Plan

> **Status**: âœ… **Core Feature Complete and Tested**
> - Phases 1-4 fully implemented and working
> - Successfully tested end-to-end with Python client
> - Detected 5 bounding boxes from live camera feed
> - Remaining work: Formal testing, performance benchmarks, documentation

> **Revision**: Plan updated to remove `actor_class_name` field and skip Phase 2.
> Using existing `UTempoActorLabeler::GetInstanceToSemanticIdMap()` API instead of adding new lookup methods.

## Overview

### Goal
Add native 2D bounding box computation to Tempo camera sensors, enabling efficient object detection without requiring downstream processing of semantic segmentation images.

### Problem Statement
Current workflow requires:
1. Stream semantic segmentation images to client
2. Client processes images to compute bounding boxes
3. This is slow due to network transfer and CPU processing overhead

### Proposed Solution
Compute bounding boxes directly in the Tempo plugin:
- Use instance segmentation mode (each object gets unique ID)
- Extract bounding boxes from label images
- Stream color images + bounding boxes together via gRPC
- **CPU implementation first** (Phases 1-5) for simplicity and validation
- **GPU optimization later** (Phase 6) for performance

### Success Metrics
- âœ… Bounding boxes accurately match object positions (pixel-perfect)
- âœ… Instance IDs correctly mapped to semantic classes
- âœ… Streaming API maintains frame synchronization with color images
- âœ… CPU implementation handles 20+ objects at 30+ FPS (1920x1080)
- âœ… GPU implementation achieves 2-5x performance improvement over CPU

---

## Architecture Summary

### Current System (from exploration)
```
Camera Capture â†’ Post-Process Material â†’ GPU Readback
                      â†“
            Packs RGB + Label into pixels
                      â†“
         DecodeAndRespond() on CPU
                      â†“
         Separate streams: ColorImage, LabelImage, DepthImage
```

### New System (with Bounding Boxes)
```
Camera Capture â†’ Post-Process Material â†’ GPU Readback
                      â†“
            Packs RGB + Label into pixels
                      â†“
         DecodeAndComputeBoundingBoxes() on CPU/GPU
                      â†“
         New stream: ColorImageWithBoundingBoxes
         (contains color image + array of bbox metadata)
```

### Key Components
1. **Proto Messages** (`Camera.proto`): Define BoundingBox2D and ColorImageWithBoundingBoxes
2. **UTempoActorLabeler** (`TempoLabels`): Use existing GetInstanceToSemanticIdMap() to lookup semantic IDs
3. **UTempoCamera** (`TempoCamera`): Compute bounding boxes from label image
4. **UTempoSensorServiceSubsystem** (`TempoSensors`): Stream combined messages via gRPC

---

## Phase 1: Proto Definitions & API Design

### Objectives
- Define data structures for bounding boxes
- Add new gRPC streaming RPC
- Ensure compatibility with existing sensor APIs

### Tasks

#### 1.1 Update Camera.proto
**File**: `TempoSensors/Source/TempoCamera/Public/Camera.proto`

- [x] Add `BoundingBox2D` message
  ```protobuf
  message BoundingBox2D {
    uint32 min_x = 1;        // Top-left corner X (pixels)
    uint32 min_y = 2;        // Top-left corner Y (pixels)
    uint32 max_x = 3;        // Bottom-right corner X (pixels)
    uint32 max_y = 4;        // Bottom-right corner Y (pixels)
    uint32 instance_id = 5;  // Instance ID (1-255)
    uint32 semantic_id = 6;  // Semantic class ID
    // REMOVED: actor_class_name field (not needed - instance_id + semantic_id sufficient)
  }
  ```

- [x] Add `ColorImageWithBoundingBoxes` message
  ```protobuf
  message ColorImageWithBoundingBoxes {
    ColorImage color_image = 1;
    repeated BoundingBox2D bounding_boxes = 2;
  }
  ```

#### 1.2 Update Sensors.proto
**File**: `TempoSensors/Source/TempoSensors/Public/Sensors.proto`

- [x] Add new streaming RPC to `SensorService`
  ```protobuf
  rpc StreamColorImagesWithBoundingBoxes(SensorRequest) returns (stream ColorImageWithBoundingBoxes) {}
  ```

#### 1.3 Build and Verify Proto Generation
- [x] Run build to regenerate proto headers/Python code
- [x] Verify `.pb.h` files generated in `Intermediate/Build/`
- [x] Verify Python `_pb2.py` files generated in `TempoEnv/`

### Testing
- [x] Verify proto files compile without errors
- [x] Check Python can import new message types: `from tempo.tempo_camera import Camera_pb2`

---

## Phase 2: ~~Actor Metadata Lookup~~ **SKIPPED**

### Decision: Not Necessary

After investigating the existing TempoLabels API, we found:
- `UTempoActorLabeler::GetInstanceToSemanticIdMap()` already provides instance ID â†’ semantic ID mappings
- For bounding boxes, we only need instance_id + semantic_id (not actor class names)
- Removing the `actor_class_name` field from BoundingBox2D proto eliminates the need for Phase 2

**Phase 2 is skipped entirely.** We'll use the existing API in Phase 3.

---

## Phase 3: CPU Bounding Box Computation

### Objectives
- Extract bounding boxes from label images on CPU
- Integrate with existing camera rendering pipeline
- Maintain async processing for performance

### Tasks

#### 3.1 Update Camera Measurement Types
**File**: `TempoSensors/Source/TempoCamera/Public/TempoCamera.h`

- [ ] Add new measurement type to `ECameraMeasurementType` enum (around line 162):
  ```cpp
  enum class ECameraMeasurementType : uint8
  {
      COLOR_IMAGE,
      LABEL_IMAGE,
      DEPTH_IMAGE,
      COLOR_IMAGE_WITH_BBOXES  // NEW
  };
  ```

- [ ] Update `GetMeasurementTypes()` to include new type when requested

#### 3.2 Add Bounding Box Data Structures
**File**: `TempoSensors/Source/TempoCamera/Private/TempoCamera.cpp`

- [ ] Add struct for intermediate bbox data (add near top of file):
  ```cpp
  struct FBoundingBox2D
  {
      uint32 MinX = TNumericLimits<uint32>::Max();
      uint32 MinY = TNumericLimits<uint32>::Max();
      uint32 MaxX = 0;
      uint32 MaxY = 0;
      uint8 InstanceId = 0;

      bool IsValid() const { return MinX <= MaxX && MinY <= MaxY; }

      void Expand(uint32 X, uint32 Y)
      {
          MinX = FMath::Min(MinX, X);
          MinY = FMath::Min(MinY, Y);
          MaxX = FMath::Max(MaxX, X);
          MaxY = FMath::Max(MaxY, Y);
      }
  };
  ```

#### 3.3 Implement CPU Bounding Box Computation
**File**: `TempoSensors/Source/TempoCamera/Private/TempoCamera.cpp`

- [ ] Add function to compute bounding boxes from label buffer:
  ```cpp
  /**
   * Compute 2D bounding boxes from label image buffer.
   * Single-pass algorithm: scan all pixels, maintain min/max coords per instance ID.
   */
  static TArray<FBoundingBox2D> ComputeBoundingBoxesCPU(
      const TArray<uint8>& LabelData,
      uint32 Width,
      uint32 Height)
  {
      TRACE_CPUPROFILER_EVENT_SCOPE(ComputeBoundingBoxesCPU);

      // Array indexed by instance ID (0-255)
      FBoundingBox2D Boxes[256];
      for (int i = 0; i < 256; i++)
      {
          Boxes[i].InstanceId = i;
      }

      // Single pass: scan all pixels
      for (uint32 Y = 0; Y < Height; Y++)
      {
          for (uint32 X = 0; X < Width; X++)
          {
              const uint8 InstanceId = LabelData[Y * Width + X];
              if (InstanceId > 0)  // 0 = unlabeled
              {
                  Boxes[InstanceId].Expand(X, Y);
              }
          }
      }

      // Filter valid boxes
      TArray<FBoundingBox2D> ValidBoxes;
      ValidBoxes.Reserve(255);  // Scene can have up to 255 bounding boxes
      for (int i = 1; i < 256; i++)  // Skip 0 (unlabeled)
      {
          if (Boxes[i].IsValid())
          {
              ValidBoxes.Add(Boxes[i]);
          }
      }

      return ValidBoxes;
  }
  ```

#### 3.4 Implement Proto Message Construction
**File**: `TempoSensors/Source/TempoCamera/Private/TempoCamera.cpp`

- [ ] Add function to build ColorImageWithBoundingBoxes proto message:
  ```cpp
  static void BuildBoundingBoxMessage(
      const ColorImage& ColorImageMsg,
      const TArray<FBoundingBox2D>& Boxes,
      UTempoActorLabeler* Labeler,
      ColorImageWithBoundingBoxes& OutMessage)
  {
      // Copy color image
      *OutMessage.mutable_color_image() = ColorImageMsg;

      // Get instance-to-semantic mapping from UTempoActorLabeler
      TempoLabels::InstanceToSemanticIdMap InstanceToSemanticMap;
      if (Labeler)
      {
          // Use existing API to get instanceâ†’semantic mappings
          Labeler->GetInstanceToSemanticIdMap(TempoScripting::Empty(),
              TResponseDelegate<TempoLabels::InstanceToSemanticIdMap>::CreateLambda(
                  [&InstanceToSemanticMap](const TempoLabels::InstanceToSemanticIdMap& Response, const grpc::Status&)
                  {
                      InstanceToSemanticMap = Response;
                  }
              )
          );
      }

      // Add bounding boxes
      for (const FBoundingBox2D& Box : Boxes)
      {
          BoundingBox2D* BBoxProto = OutMessage.add_bounding_boxes();
          BBoxProto->set_min_x(Box.MinX);
          BBoxProto->set_min_y(Box.MinY);
          BBoxProto->set_max_x(Box.MaxX);
          BBoxProto->set_max_y(Box.MaxY);
          BBoxProto->set_instance_id(Box.InstanceId);

          // Find semantic ID from mapping
          uint8 SemanticId = 0;
          for (const auto& Pair : InstanceToSemanticMap.instance_semantic_id_pairs())
          {
              if (Pair.instanceid() == Box.InstanceId)
              {
                  SemanticId = Pair.semanticid();
                  break;
              }
          }
          BBoxProto->set_semantic_id(SemanticId);
      }
  }
  ```

#### 3.5 Integrate into Camera Rendering Pipeline
**File**: `TempoSensors/Source/TempoCamera/Private/TempoCamera.cpp`

- [ ] Modify `SendMeasurements()` to handle COLOR_IMAGE_WITH_BBOXES requests
  - Capture both color and label images
  - Queue both textures for readback
  - Dispatch async task after readback completes

- [ ] Add async function `DecodeAndComputeBoundingBoxes()`:
  - Extract label data from pixel buffer
  - Call `ComputeBoundingBoxesCPU()`
  - Call `UTempoActorLabeler::GetInstanceToSemanticIdMap()` (existing API)
  - Build ColorImageWithBoundingBoxes proto message
  - Invoke gRPC response callback

- [ ] Ensure proper lifetime management of texture data and callbacks

### Testing
- [ ] Unit test: `ComputeBoundingBoxesCPU()` with synthetic label images
  - Test single object (should return 1 bbox)
  - Test multiple objects (should return N bboxes)
  - Test empty image (should return 0 bboxes)
  - Test objects at image edges (verify boundary handling)
- [ ] Integration test: Capture camera frame, verify bboxes computed correctly
- [ ] Performance test: Measure CPU time for various image sizes (512x512, 1920x1080, 3840x2160)

---

## Phase 4: gRPC Service Implementation

### Objectives
- Expose new streaming RPC to clients
- Ensure frame synchronization between color images and bounding boxes
- Follow existing streaming patterns

### Tasks

#### 4.1 Implement StreamColorImagesWithBoundingBoxes RPC
**File**: `TempoSensors/Source/TempoSensors/Private/TempoSensorServiceSubsystem.cpp`

- [ ] Add RPC handler method to `UTempoSensorServiceSubsystem`:
  ```cpp
  grpc::Status StreamColorImagesWithBoundingBoxes(
      grpc::ServerContext* Context,
      const SensorRequest* Request,
      grpc::ServerWriter<ColorImageWithBoundingBoxes>* Writer)
  {
      return RequestImages<ColorImageWithBoundingBoxes>(
          Context,
          Request,
          Writer,
          ECameraMeasurementType::COLOR_IMAGE_WITH_BBOXES);
  }
  ```

- [ ] Verify `RequestImages()` template handles new message type correctly
- [ ] Update service builder to register new RPC

#### 4.2 Update Available Sensors Response
**File**: `TempoSensors/Source/TempoSensors/Private/TempoSensorServiceSubsystem.cpp`

- [ ] Add `COLOR_IMAGE_WITH_BBOXES` to measurement types returned by `GetAvailableSensors()`

### Testing
- [ ] Python integration test: Connect to gRPC service, call `GetAvailableSensors()`
  - Verify `COLOR_IMAGE_WITH_BBOXES` appears in capabilities
- [ ] Python streaming test: Call `StreamColorImagesWithBoundingBoxes()`
  - Verify stream opens successfully
  - Verify messages received each frame
  - Verify color image + bboxes are synchronized (same sequence_id)

---

## Phase 5: End-to-End Testing & Validation

### Objectives
- Validate correctness of bounding box computation
- Verify API usability from Python clients
- Establish performance baseline for CPU implementation

### Test Scenarios

#### 5.1 Correctness Tests

- [ ] **Test 1: Single Static Vehicle**
  - Setup: Place one labeled vehicle in scene
  - Expected: 1 bounding box, correct position, correct class name
  - Validation: Manually verify bbox coordinates match object in color image

- [ ] **Test 2: Multiple Objects of Different Classes**
  - Setup: Place 5 different labeled objects (vehicle, pedestrian, building, etc.)
  - Expected: 5 bounding boxes, each with correct semantic ID and class name
  - Validation: Verify instance IDs unique, semantic IDs correct per class

- [ ] **Test 3: Partially Occluded Objects**
  - Setup: Place objects overlapping in camera view
  - Expected: Bounding boxes for all visible instances (may overlap)
  - Validation: Each instance gets separate bbox even if occluded

- [ ] **Test 4: Objects at Image Boundaries**
  - Setup: Position objects partially outside camera frustum
  - Expected: Bounding boxes clipped to image bounds (0 to width-1, 0 to height-1)
  - Validation: No out-of-bounds coordinates

- [ ] **Test 5: Maximum Instance Count**
  - Setup: Spawn 255 labeled objects
  - Expected: 255 bounding boxes, all correctly identified
  - Validation: No ID collisions, all objects detected

- [ ] **Test 6: Moving Objects**
  - Setup: Vehicles driving through scene
  - Expected: Bounding boxes update each frame, track object movement
  - Validation: Bbox positions change smoothly, no dropped frames

#### 5.2 API Usability Tests

- [ ] **Python Client Test Script**
  ```python
  import tempo.tempo_sensors as ts
  import tempo.tempo_camera as tc

  # Connect and stream
  for msg in ts.stream_color_images_with_bounding_boxes(
      owner_name="PlayerCameraManager",
      sensor_name="Camera"):

      print(f"Frame {msg.color_image.header.sequence_id}")
      print(f"  Found {len(msg.bounding_boxes)} objects")

      for bbox in msg.bounding_boxes:
          print(f"  - {bbox.actor_class_name}: "
                f"({bbox.min_x},{bbox.min_y}) to ({bbox.max_x},{bbox.max_y})")
  ```

- [ ] Verify Python API ergonomics (easy to use, well-typed)
- [ ] Test error handling (invalid sensor name, wrong mode, etc.)

#### 5.3 Performance Tests

- [ ] **Benchmark CPU Implementation**
  - Test configurations:
    - 1920x1080 resolution, 10 objects
    - 1920x1080 resolution, 50 objects
    - 3840x2160 resolution (4K), 20 objects
  - Metrics to measure:
    - Bbox computation time (CPU ms)
    - Frame time impact (total ms)
    - FPS at 30Hz, 60Hz target rates
  - Acceptance criteria: <5ms bbox computation for 1080p with 20 objects

- [ ] **Profile Memory Usage**
  - Verify no memory leaks over 1000+ frames
  - Check texture buffer pooling works correctly

### Documentation

- [ ] Update `CLAUDE.md` with bounding box feature:
  - Add section on bounding box streaming
  - Document instance mode requirement
  - Add Python usage example

- [ ] Add inline code comments explaining algorithms

---

## Phase 6: GPU Optimization âœ… **IMPLEMENTED**

### Objectives
- âœ… Accelerate bounding box computation using GPU compute shaders
- ðŸ”„ Achieve 2-5x speedup over CPU implementation (to be benchmarked)
- ðŸ”„ Maintain identical results to CPU (to be validated)

### Tasks

#### 6.1 Implement Compute Shader
**File**: `TempoSensors/Content/Shaders/BoundingBoxExtraction.usf` (new)

- [ ] Create compute shader with two passes:
  - **Pass 1**: Parallel scan, atomic min/max per instance ID
  - **Pass 2**: Compaction pass to remove empty instances

- [ ] Implement thread group optimization (8x8 or 16x16 groups)

- [ ] Add shader parameters:
  - Input: Label texture (SRV)
  - Output: Structured buffer of bboxes (UAV)
  - Constants: Width, Height

#### 6.2 Integrate Compute Dispatch
**File**: `TempoSensors/Source/TempoCamera/Private/TempoCamera.cpp`

- [ ] Add compute shader dispatch:
  ```cpp
  void UTempoCamera::ComputeBoundingBoxesGPU(
      FRHICommandListImmediate& RHICmdList,
      FRHITexture* LabelTexture,
      FRHIStructuredBuffer* OutputBuffer)
  {
      // Set shader and parameters
      // Dispatch compute: NumGroupsX = (Width + 7) / 8, NumGroupsY = (Height + 7) / 8
      // Add UAV barrier
      // Queue readback of output buffer
  }
  ```

- [ ] Add GPUâ†’CPU readback for bbox buffer (similar to texture readback)

- [ ] Add fallback: Use CPU if compute shader fails to compile

#### 6.3 Performance Comparison
- [ ] Benchmark GPU vs CPU for same test cases
- [ ] Measure speedup factor
- [ ] Profile GPU occupancy and memory bandwidth

### Success Criteria
- [ ] GPU implementation produces same bboxes as CPU (within Â±1 pixel tolerance)
- [ ] GPU achieves 2-5x faster computation time
- [ ] No regressions in frame rate or memory usage

---

## Implementation Checklist Summary

### Phase 1: Proto Definitions
- [x] Add BoundingBox2D message to Camera.proto
- [x] Add ColorImageWithBoundingBoxes message to Camera.proto
- [x] Add StreamColorImagesWithBoundingBoxes RPC to Sensors.proto
- [x] Build and verify proto generation
- [x] Remove actor_class_name field from BoundingBox2D proto
- [x] Rebuild to regenerate proto files without actor_class_name

### Phase 2: ~~Metadata Lookup~~ **SKIPPED**
- [x] ~~Add GetActorClassNameForInstanceId() to UTempoActorLabeler header~~ Not needed
- [x] ~~Implement GetActorClassNameForInstanceId() in .cpp~~ Not needed
- [x] ~~Add unit tests for lookup function~~ Not needed
- **Using existing `UTempoActorLabeler::GetInstanceToSemanticIdMap()` API instead**

### Phase 3: CPU Computation
- [x] Add COLOR_IMAGE_WITH_BBOXES to ECameraMeasurementType
- [x] Implement ComputeBoundingBoxesCPU() function
- [x] Implement BuildBoundingBoxMessage() function (RespondToBoundingBoxRequests)
- [x] Integrate into camera rendering pipeline
- [x] Add DecodeAndComputeBoundingBoxes() async function (integrated in DecodeAndRespond)
- [ ] Add unit tests for bbox computation
- [ ] Add performance benchmarks

### Phase 4: gRPC Service
- [x] Implement StreamColorImagesWithBoundingBoxes() RPC handler
- [x] Register RPC in service builder
- [x] Add RequestMeasurement() overload to UTempoCamera
- [x] Update GetAvailableSensors() response (automatic via MeasurementTypes)
- [x] Add Python integration tests (verified working with 5 bounding boxes detected)

### Phase 5: Testing & Validation
- [x] Verify Python API usability (successfully tested end-to-end)
- [x] Basic integration test (detected 5 objects with bounding boxes from TempoCamera)
- [ ] Run all correctness test scenarios (6 formal tests)
- [ ] Run performance benchmarks
- [ ] Update documentation

### Phase 6: GPU Optimization
- [x] Implement compute shader (BoundingBoxExtraction.usf)
- [x] Create shader C++ wrapper (BoundingBoxComputeShader.h/cpp)
- [x] Integrate GPU dispatch in UpdateSceneCaptureContents()
- [x] Add buffer readback and CPU fallback
- [ ] Benchmark GPU vs CPU performance
- [ ] Validate GPU results match CPU

---

## Risk Assessment

### Technical Risks

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| CPU performance insufficient | Medium | High | Phase 6 GPU optimization planned |
| Instance ID limit (255) too restrictive | Low | Medium | Document limitation, consider future expansion to 16-bit IDs |
| Thread safety issues with labeler access | Low | Low | Use existing thread-safe GetInstanceToSemanticIdMap() API |
| Proto changes break existing clients | Low | High | Additive changes only, no breaking modifications |

### Schedule Risks

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| GPU shader complexity exceeds estimate | High | Medium | Phase 6 is optional, CPU version sufficient for initial release |
| Testing reveals edge cases | Medium | Low | Comprehensive test suite in Phase 5 |

---

## Success Criteria

### Must Have (MVP)
âœ… CPU implementation computes accurate bounding boxes
âœ… Bboxes include instance ID and semantic ID
âœ… gRPC streaming API delivers color image + bboxes in single message
âœ… Python client can consume API easily
âœ… Performance acceptable for typical scenes (20 objects @ 1080p @ 30 FPS)

### Should Have
âœ… Comprehensive test coverage
âœ… Performance benchmarks documented
âœ… Code properly commented and maintainable

### Nice to Have
âœ… GPU optimization (Phase 6)
âœ… Pixel coverage metadata
âœ… Depth information per bbox

---

## Timeline Estimate

| Phase | Duration | Dependencies |
|-------|----------|--------------|
| Phase 1: Proto | 0.5 days | None |
| ~~Phase 2: Metadata~~ | **SKIPPED** | N/A |
| Phase 3: CPU Computation | 2 days | Phase 1 |
| Phase 4: gRPC Service | 1 day | Phase 3 |
| Phase 5: Testing | 1-2 days | Phase 4 |
| **Total (CPU MVP)** | **4.5-5.5 days** | |
| Phase 6: GPU (Optional) | 2-3 days | Phase 5 |

---

## Notes

- Ensure `LabelType` is set to `Instance` in TempoSensorsSettings (required for unique IDs)
- Bounding boxes are axis-aligned only (no rotation)
- Occlusion not handled: bbox includes all pixels with that instance ID, even if partially occluded
- 255 instance ID limit is hardware constraint (8-bit stencil buffer)
- Consider future enhancement: 16-bit instance IDs using custom render target (65535 objects)
